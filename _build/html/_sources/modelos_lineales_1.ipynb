{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a75981b",
   "metadata": {},
   "source": [
    "# Regresión Lineal Simple: Teoría y Práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2d0bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importamos las bibliotecas necesarias\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Configuración básica de matplotlib\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = [10, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3fe21",
   "metadata": {},
   "source": [
    "## 1. El Modelo Matemático\n",
    "\n",
    "```{admonition} Fundamentos\n",
    ":class: important\n",
    "\n",
    "La regresión lineal simple es un método estadístico que modeliza la relación entre una variable dependiente (y) y una variable independiente (x). Es \"simple\" porque solo hay una variable independiente y \"lineal\" porque el modelo es lineal en los parámetros.\n",
    "```\n",
    "\n",
    "### 1.1 Definición Formal\n",
    "\n",
    "El modelo se expresa como:\n",
    "\n",
    "```{math}\n",
    ":label: modelo-base\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i = 1, 2, ..., n\n",
    "```\n",
    "\n",
    "donde:\n",
    "- $y_i$ es la variable dependiente (respuesta)\n",
    "- $x_i$ es la variable independiente (predictora)\n",
    "- $\\beta_0$ es el intercepto\n",
    "- $\\beta_1$ es la pendiente\n",
    "- $\\epsilon_i$ es el término de error aleatorio\n",
    "\n",
    "### 1.2 Supuestos del Modelo\n",
    "\n",
    "Los supuestos fundamentales son:\n",
    "\n",
    "```{list-table} Supuestos Estadísticos\n",
    ":header-rows: 1\n",
    ":name: tabla-supuestos\n",
    "\n",
    "* - Supuesto\n",
    "  - Expresión Matemática\n",
    "  - Interpretación\n",
    "* - 1\n",
    "  - $E(\\epsilon_i) = 0$\n",
    "  - El modelo es correcto y $E(y_i) = \\beta_0 + \\beta_1 x_i$\n",
    "* - 2\n",
    "  - $var(\\epsilon_i) = \\sigma^2$\n",
    "  - Homocedasticidad (varianza constante)\n",
    "* - 3\n",
    "  - $cov(\\epsilon_i, \\epsilon_j) = 0$\n",
    "  - Errores no correlacionados\n",
    "```\n",
    "\n",
    "Veamos un ejemplo con datos simulados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f399fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos que cumplen los supuestos\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "x = np.linspace(0, 10, n)\n",
    "epsilon = np.random.normal(0, 1.5, n)  # Errores normales independientes\n",
    "beta0_true, beta1_true = 2, 3\n",
    "y = beta0_true + beta1_true * x + epsilon\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(x, y, alpha=0.5, label='Datos observados')\n",
    "plt.plot(x, beta0_true + beta1_true * x, 'r--', label='Relación verdadera')\n",
    "plt.xlabel('Variable independiente (x)')\n",
    "plt.ylabel('Variable dependiente (y)')\n",
    "plt.title('Ejemplo de Regresión Lineal Simple')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d89abf",
   "metadata": {},
   "source": [
    "# 2. Estimación de Parámetros\n",
    "\n",
    "## 2.1 Método de Mínimos Cuadrados\n",
    "\n",
    "```{admonition} Principio de Mínimos Cuadrados\n",
    "El método busca los valores de $\\beta_0$ y $\\beta_1$ que minimizan la suma de cuadrados de los residuos:\n",
    "\n",
    "$\\sum_{i=1}^n (y_i - \\hat{y}i)^2 = \\sum{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2$\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Objetivo\n",
    ":class: important\n",
    "Encontrar los valores de $\\beta_0$ y $\\beta_1$ que minimicen la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos.\n",
    "```\n",
    "\n",
    "Definimos la función objetivo:\n",
    "\n",
    "```{math}\n",
    ":label: eq-objetivo\n",
    "S(\\beta_0, \\beta_1) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "```\n",
    "\n",
    "## 2.2 Condiciones de Primer Orden\n",
    "\n",
    "Para minimizar $S(\\beta_0, \\beta_1)$, necesitamos que:\n",
    "\n",
    "```{math}\n",
    ":label: eq-condiciones\n",
    "\\begin{align*}\n",
    "\\frac{\\partial S}{\\partial \\beta_0} &= 0 \\\\\n",
    "\\frac{\\partial S}{\\partial \\beta_1} &= 0\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "### 2.2.1 Primera Derivada Parcial\n",
    "\n",
    "Desarrollamos $\\frac{\\partial S}{\\partial \\beta_0}$:\n",
    "\n",
    "```{math}\n",
    ":label: eq-derivada-beta0\n",
    "\\begin{align*}\n",
    "\\frac{\\partial S}{\\partial \\beta_0} &= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n",
    "&= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_0} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n",
    "&= \\sum_{i=1}^n 2(y_i - \\beta_0 - \\beta_1 x_i)(-1) \\\\\n",
    "&= -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "### 2.2.2 Segunda Derivada Parcial\n",
    "\n",
    "Desarrollamos $\\frac{\\partial S}{\\partial \\beta_1}$:\n",
    "\n",
    "```{math}\n",
    ":label: eq-derivada-beta1\n",
    "\\begin{align*}\n",
    "\\frac{\\partial S}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n",
    "&= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_1} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n",
    "&= \\sum_{i=1}^n 2(y_i - \\beta_0 - \\beta_1 x_i)(-x_i) \\\\\n",
    "&= -2\\sum_{i=1}^n x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "## 2.3 Sistema de Ecuaciones Normales\n",
    "\n",
    "De {eq}`eq-derivada-beta0`:\n",
    "```{math}\n",
    ":label: eq-normal1\n",
    "\\sum_{i=1}^n y_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^n x_i = 0\n",
    "```\n",
    "\n",
    "De {eq}`eq-derivada-beta1`:\n",
    "```{math}\n",
    ":label: eq-normal2\n",
    "\\sum_{i=1}^n x_iy_i - \\beta_0 \\sum_{i=1}^n x_i - \\beta_1 \\sum_{i=1}^n x_i^2 = 0\n",
    "```\n",
    "\n",
    "## 2.4 Solución del Sistema\n",
    "\n",
    "### 2.4.1 Obtención de $\\beta_0$\n",
    "\n",
    "De {eq}`eq-normal1`:\n",
    "```{math}\n",
    ":label: eq-beta0-1\n",
    "\\beta_0 = \\frac{\\sum_{i=1}^n y_i}{n} - \\beta_1 \\frac{\\sum_{i=1}^n x_i}{n} = \\bar{y} - \\beta_1 \\bar{x}\n",
    "```\n",
    "\n",
    "### 2.4.2 Obtención de $\\beta_1$\n",
    "\n",
    "Sustituyendo {eq}`eq-beta0-1` en {eq}`eq-normal2`:\n",
    "\n",
    "```{math}\n",
    ":label: eq-beta1-desarrollo\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n x_iy_i - (\\bar{y} - \\beta_1 \\bar{x})\\sum_{i=1}^n x_i - \\beta_1 \\sum_{i=1}^n x_i^2 &= 0 \\\\\n",
    "\\sum_{i=1}^n x_iy_i - \\bar{y}\\sum_{i=1}^n x_i + \\beta_1 \\bar{x}\\sum_{i=1}^n x_i - \\beta_1 \\sum_{i=1}^n x_i^2 &= 0\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "Reordenando términos:\n",
    "\n",
    "```{math}\n",
    ":label: eq-beta1-final\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^n x_iy_i - \\bar{y}\\sum_{i=1}^n x_i}{\\sum_{i=1}^n x_i^2 - \\bar{x}\\sum_{i=1}^n x_i}\n",
    "```\n",
    "\n",
    "### 2.4.3 Forma Alternativa de $\\beta_1$\n",
    "\n",
    "Podemos reescribir $\\beta_1$ en términos de desviaciones respecto a la media:\n",
    "\n",
    "```{math}\n",
    ":label: eq-beta1-alt\n",
    "\\begin{align*}\n",
    "\\beta_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n",
    "&= \\frac{Cov(x,y)}{Var(x)}\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "## 2.5 Verificación de Mínimo\n",
    "\n",
    "Para confirmar que hemos encontrado un mínimo y no un máximo, verificamos las segundas derivadas:\n",
    "\n",
    "```{math}\n",
    ":label: eq-segundas-derivadas\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 S}{\\partial \\beta_0^2} &= 2n > 0 \\\\\n",
    "\\frac{\\partial^2 S}{\\partial \\beta_1^2} &= 2\\sum_{i=1}^n x_i^2 > 0 \\\\\n",
    "\\frac{\\partial^2 S}{\\partial \\beta_0\\partial \\beta_1} &= 2\\sum_{i=1}^n x_i\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "La matriz Hessiana es:\n",
    "```{math}\n",
    ":label: eq-hessiana\n",
    "H = 2\\begin{bmatrix} \n",
    "n & \\sum x_i \\\\\n",
    "\\sum x_i & \\sum x_i^2\n",
    "\\end{bmatrix}\n",
    "```\n",
    "\n",
    "Es definida positiva, confirmando que hemos encontrado un mínimo global.\n",
    "\n",
    "## 2.6 Propiedades Algebraicas\n",
    "\n",
    "Las soluciones tienen las siguientes propiedades:\n",
    "\n",
    "1. $\\sum_{i=1}^n (y_i - \\hat{y}_i) = 0$\n",
    "2. $\\sum_{i=1}^n x_i(y_i - \\hat{y}_i) = 0$\n",
    "3. $\\sum_{i=1}^n \\hat{y}_i = \\sum_{i=1}^n y_i$\n",
    "4. $\\hat{y} = \\bar{y}$\n",
    "\n",
    "```{note}\n",
    "Estas propiedades son consecuencia directa de las condiciones de primer orden y son fundamentales para la inferencia estadística en regresión lineal.\n",
    "```\n",
    "\n",
    "Implementemos estas fórmulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_estimadores_mc(x, y):\n",
    "    \"\"\"Calcula los estimadores de mínimos cuadrados\"\"\"\n",
    "    x_mean, y_mean = np.mean(x), np.mean(y)\n",
    "    beta1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n",
    "    beta0 = y_mean - beta1 * x_mean\n",
    "    return beta0, beta1\n",
    "\n",
    "beta0_est, beta1_est = calcular_estimadores_mc(x, y)\n",
    "print(f'Estimadores de mínimos cuadrados:')\n",
    "print(f'β₀: {beta0_est:.4f} (verdadero: {beta0_true})')\n",
    "print(f'β₁: {beta1_est:.4f} (verdadero: {beta1_true})')\n",
    "\n",
    "# Visualización del ajuste\n",
    "plt.scatter(x, y, alpha=0.5, label='Datos')\n",
    "plt.plot(x, beta0_true + beta1_true * x, 'r--', label='Verdadera')\n",
    "plt.plot(x, beta0_est + beta1_est * x, 'g-', label='Ajustada')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Comparación de Líneas Verdadera y Ajustada')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ba861",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "Los estimadores de mínimos cuadrados son insesgados y eficientes bajo los supuestos del modelo.\n",
    "```\n",
    "\n",
    "## 3. Propiedades de los Estimadores\n",
    "\n",
    "```{admonition} Propiedades Teóricas\n",
    ":class: important\n",
    "Bajo los supuestos del modelo, los estimadores de mínimos cuadrados son:\n",
    "1. Insesgados: $E(\\hat{\\beta}_0) = \\beta_0$ y $E(\\hat{\\beta}_1) = \\beta_1$\n",
    "2. De mínima varianza entre todos los estimadores lineales insesgados\n",
    "```\n",
    "\n",
    "Las varianzas de los estimadores son:\n",
    "\n",
    "```{math}\n",
    ":label: varianzas\n",
    "\\begin{align*}\n",
    "var(\\hat{\\beta}_1) &= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n",
    "var(\\hat{\\beta}_0) &= \\sigma^2(\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2})\n",
    "\\end{align*}\n",
    "```\n",
    "## 3.1 Insesgamiento\n",
    "\n",
    "### 3.1.1 Insesgamiento de $\\hat{\\beta}_1$\n",
    "\n",
    "```{admonition} Teorema\n",
    ":class: important\n",
    "El estimador $\\hat{\\beta}_1$ es insesgado, es decir, $E(\\hat{\\beta}_1) = \\beta_1$\n",
    "```\n",
    "\n",
    "Demostración:\n",
    "\n",
    "```{math}\n",
    ":label: eq-unbiased-beta1\n",
    "\\begin{align*}\n",
    "E(\\hat{\\beta}_1) &= E\\left[\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right] \\\\\n",
    "&= \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2}E\\left[\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\right]\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "Dado que $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$:\n",
    "\n",
    "```{math}\n",
    ":label: eq-unbiased-beta1-cont\n",
    "\\begin{align*}\n",
    "E(y_i) &= \\beta_0 + \\beta_1x_i \\\\\n",
    "E(\\bar{y}) &= \\beta_0 + \\beta_1\\bar{x}\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "Sustituyendo:\n",
    "\n",
    "```{math}\n",
    ":label: eq-unbiased-beta1-final\n",
    "\\begin{align*}\n",
    "E(\\hat{\\beta}_1) &= \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sum_{i=1}^n (x_i - \\bar{x})(\\beta_0 + \\beta_1x_i - \\beta_0 - \\beta_1\\bar{x}) \\\\\n",
    "&= \\frac{1}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sum_{i=1}^n (x_i - \\bar{x})\\beta_1(x_i - \\bar{x}) \\\\\n",
    "&= \\beta_1\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\beta_1\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "### 3.1.2 Insesgamiento de $\\hat{\\beta}_0$\n",
    "\n",
    "```{admonition} Teorema\n",
    ":class: important\n",
    "El estimador $\\hat{\\beta}_0$ es insesgado, es decir, $E(\\hat{\\beta}_0) = \\beta_0$\n",
    "```\n",
    "\n",
    "Demostración:\n",
    "\n",
    "```{math}\n",
    ":label: eq-unbiased-beta0\n",
    "\\begin{align*}\n",
    "E(\\hat{\\beta}_0) &= E(\\bar{y} - \\hat{\\beta}_1\\bar{x}) \\\\\n",
    "&= E(\\bar{y}) - \\bar{x}E(\\hat{\\beta}_1) \\\\\n",
    "&= (\\beta_0 + \\beta_1\\bar{x}) - \\bar{x}\\beta_1 \\\\\n",
    "&= \\beta_0\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "## 3.2 Varianzas de los Estimadores\n",
    "\n",
    "### 3.2.1 Varianza de $\\hat{\\beta}_1$\n",
    "\n",
    "```{math}\n",
    ":label: eq-var-beta1\n",
    "\\begin{align*}\n",
    "var(\\hat{\\beta}_1) &= var\\left[\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right] \\\\\n",
    "&= \\frac{1}{[\\sum_{i=1}^n (x_i - \\bar{x})^2]^2}var\\left[\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\right]\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "Dado que $var(y_i) = \\sigma^2$ y los errores son independientes:\n",
    "\n",
    "```{math}\n",
    ":label: eq-var-beta1-final\n",
    "var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "```\n",
    "\n",
    "### 3.2.2 Varianza de $\\hat{\\beta}_0$\n",
    "\n",
    "```{math}\n",
    ":label: eq-var-beta0\n",
    "\\begin{align*}\n",
    "var(\\hat{\\beta}_0) &= var(\\bar{y} - \\hat{\\beta}_1\\bar{x}) \\\\\n",
    "&= var(\\bar{y}) + \\bar{x}^2var(\\hat{\\beta}_1) - 2\\bar{x}cov(\\bar{y},\\hat{\\beta}_1)\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "Después de desarrollar los términos:\n",
    "\n",
    "```{math}\n",
    ":label: eq-var-beta0-final\n",
    "var(\\hat{\\beta}_0) = \\sigma^2(\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2})\n",
    "```\n",
    "\n",
    "## 3.3 Teorema de Gauss-Markov\n",
    "\n",
    "```{admonition} Teorema de Gauss-Markov\n",
    ":class: important\n",
    "Entre todos los estimadores lineales e insesgados de $\\beta_0$ y $\\beta_1$, los estimadores de mínimos cuadrados tienen varianza mínima.\n",
    "```\n",
    "\n",
    "### 3.3.1 Demostración del Teorema\n",
    "\n",
    "Sea $\\tilde{\\beta}_1$ otro estimador lineal insesgado de $\\beta_1$:\n",
    "\n",
    "```{math}\n",
    ":label: eq-gauss-markov\n",
    "\\begin{align*}\n",
    "\\tilde{\\beta}_1 &= \\sum_{i=1}^n a_iy_i \\\\\n",
    "E(\\tilde{\\beta}_1) &= \\beta_1 \\implies \\sum_{i=1}^n a_ix_i = 1 \\text{ y } \\sum_{i=1}^n a_i = 0\n",
    "\\end{align*}\n",
    "```\n",
    "\n",
    "Entonces:\n",
    "\n",
    "```{math}\n",
    ":label: eq-gauss-markov-var\n",
    "var(\\tilde{\\beta}_1) = \\sigma^2\\sum_{i=1}^n a_i^2 \\geq \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = var(\\hat{\\beta}_1)\n",
    "```\n",
    "\n",
    "## 3.4 Propiedades de los Residuos\n",
    "\n",
    "Los residuos $\\hat{\\epsilon}_i = y_i - \\hat{y}_i$ tienen las siguientes propiedades:\n",
    "\n",
    "1. $\\sum_{i=1}^n \\hat{\\epsilon}_i = 0$\n",
    "2. $\\sum_{i=1}^n x_i\\hat{\\epsilon}_i = 0$\n",
    "3. $\\sum_{i=1}^n \\hat{y}_i\\hat{\\epsilon}_i = 0$\n",
    "4. $Cov(\\hat{y}_i, \\hat{\\epsilon}_i) = 0$\n",
    "\n",
    "```{note}\n",
    "Estas propiedades son fundamentales para la inferencia estadística y la validación del modelo.\n",
    "```\n",
    "\n",
    "## 3.5 Consecuencias Prácticas\n",
    "\n",
    "1. **Eficiencia**: Los estimadores MCO son los mejores estimadores lineales insesgados (BLUE).\n",
    "\n",
    "2. **Precisión**:\n",
    "   - La precisión de $\\hat{\\beta}_1$ aumenta con:\n",
    "     * Mayor variabilidad en x ($\\sum_{i=1}^n (x_i - \\bar{x})^2$ grande)\n",
    "     * Menor varianza del error ($\\sigma^2$ pequeño)\n",
    "     * Mayor tamaño muestral (n)\n",
    "\n",
    "3. **Intervalo de Confianza**:\n",
    "   - Los intervalos de confianza serán más estrechos cuando:\n",
    "     * Las varianzas sean menores\n",
    "     * El tamaño muestral sea mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b70daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación para verificar insesgamiento\n",
    "n_sims = 1000\n",
    "beta0_sims = np.zeros(n_sims)\n",
    "beta1_sims = np.zeros(n_sims)\n",
    "\n",
    "for i in range(n_sims):\n",
    "    epsilon = np.random.normal(0, 1.5, n)\n",
    "    y_sim = beta0_true + beta1_true * x + epsilon\n",
    "    beta0_sims[i], beta1_sims[i] = calcular_estimadores_mc(x, y_sim)\n",
    "\n",
    "print(\"Resultados de simulación:\")\n",
    "print(f'E(β₀): {np.mean(beta0_sims):.4f} (verdadero: {beta0_true})')\n",
    "print(f'E(β₁): {np.mean(beta1_sims):.4f} (verdadero: {beta1_true})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8575c29",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "Los resultados de la simulación confirman que los estimadores de mínimos cuadrados son insesgados.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación para verificar varianzas\n",
    "sigma2 = 1.5**2\n",
    "var_beta0 = sigma2 * (1/n + np.mean(x)**2 / np.sum((x - np.mean(x))**2))\n",
    "var_beta1 = sigma2 / np.sum((x - np.mean(x))**2)\n",
    "\n",
    "print(\"Varianzas de los estimadores:\")\n",
    "print(f'var(β₀): {var_beta0:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec006a",
   "metadata": {},
   "source": [
    "## 4. Inferencia Estadística en Regresión Lineal\n",
    "\n",
    "### 4.1 Estimación de la Varianza (σ²)\n",
    "\n",
    "La varianza del error (σ²) es un parámetro crucial que mide la dispersión de los datos alrededor de la línea de regresión. Su estimación es fundamental para:\n",
    "- Calcular errores estándar de los coeficientes\n",
    "- Construir intervalos de confianza\n",
    "- Realizar pruebas de hipótesis\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def estimate_variance(y, y_pred, n_params=2):\n",
    "    \"\"\"\n",
    "    Estima la varianza del error en regresión lineal.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Valores observados\n",
    "    y_pred : array-like\n",
    "        Valores predichos\n",
    "    n_params : int\n",
    "        Número de parámetros en el modelo (default=2 para regresión simple)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    float\n",
    "        Estimación de σ²\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    residuals = y - y_pred\n",
    "    sigma2 = np.sum(residuals**2) / (n - n_params)\n",
    "    return sigma2\n",
    "\n",
    "# Ejemplo de uso\n",
    "y_pred = beta0_est + beta1_est * x\n",
    "sigma2_est = estimate_variance(y, y_pred)\n",
    "print(f'σ² estimado: {sigma2_est:.4f}')\n",
    "```\n",
    "\n",
    "```python\n",
    "# Visualización de residuos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, y - y_pred, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Valores predichos')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title('Gráfico de Residuos vs Valores Predichos')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 4.2 Pruebas de Hipótesis\n",
    "\n",
    "Las pruebas de hipótesis nos permiten evaluar la significancia estadística de los coeficientes.\n",
    "\n",
    "```python\n",
    "def test_beta1(beta1, x, sigma2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Realiza prueba de hipótesis para β₁\n",
    "    \n",
    "    H₀: β₁ = 0 \n",
    "    H₁: β₁ ≠ 0\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    beta1 : float\n",
    "        Estimación de β₁\n",
    "    x : array-like\n",
    "        Variable independiente\n",
    "    sigma2 : float\n",
    "        Varianza estimada\n",
    "    alpha : float\n",
    "        Nivel de significancia\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict\n",
    "        Resultados del test\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    # Error estándar\n",
    "    se = np.sqrt(sigma2 / np.sum((x - np.mean(x))**2))\n",
    "    \n",
    "    # Estadístico t\n",
    "    t_stat = beta1 / se\n",
    "    \n",
    "    # Grados de libertad\n",
    "    df = n - 2\n",
    "    \n",
    "    # Valor p\n",
    "    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=df))\n",
    "    \n",
    "    # Intervalo de confianza\n",
    "    t_crit = stats.t.ppf(1-alpha/2, df=df)\n",
    "    ci = (beta1 - t_crit * se, beta1 + t_crit * se)\n",
    "    \n",
    "    return {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'confidence_interval': ci\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso\n",
    "results = test_beta1(beta1_est, x, sigma2_est)\n",
    "print(f\"Estadístico t: {results['t_statistic']:.4f}\")\n",
    "print(f\"Valor p: {results['p_value']:.4e}\")\n",
    "print(f\"IC 95%: [{results['confidence_interval'][0]:.4f}, {results['confidence_interval'][1]:.4f}]\")\n",
    "```\n",
    "\n",
    "### 4.3 Intervalos de Predicción\n",
    "\n",
    "Los intervalos de predicción proporcionan un rango para futuras observaciones.\n",
    "\n",
    "```python\n",
    "def prediction_interval(x_new, x, beta0, beta1, sigma2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calcula intervalo de predicción para nuevas observaciones\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    x_new : float o array-like\n",
    "        Nuevos valores de x para predicción\n",
    "    x : array-like\n",
    "        Valores originales de x\n",
    "    beta0, beta1 : float\n",
    "        Coeficientes estimados\n",
    "    sigma2 : float\n",
    "        Varianza estimada\n",
    "    alpha : float\n",
    "        Nivel de significancia\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    tuple\n",
    "        (predicción, límite inferior, límite superior)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    \n",
    "    # Predicción puntual\n",
    "    y_pred = beta0 + beta1 * x_new\n",
    "    \n",
    "    # Error estándar de predicción\n",
    "    se_pred = np.sqrt(sigma2 * (1 + 1/n + \n",
    "                     (x_new - x_mean)**2 / np.sum((x - x_mean)**2)))\n",
    "    \n",
    "    # Valor crítico\n",
    "    t_crit = stats.t.ppf(1-alpha/2, df=n-2)\n",
    "    \n",
    "    # Intervalos\n",
    "    pi_lower = y_pred - t_crit * se_pred\n",
    "    pi_upper = y_pred + t_crit * se_pred\n",
    "    \n",
    "    return y_pred, pi_lower, pi_upper\n",
    "\n",
    "# Ejemplo de uso\n",
    "x_new = np.linspace(min(x), max(x), 100)\n",
    "predictions = prediction_interval(x_new, x, beta0_est, beta1_est, sigma2_est)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, alpha=0.5, label='Datos observados')\n",
    "plt.plot(x_new, predictions[0], 'r-', label='Regresión')\n",
    "plt.fill_between(x_new, predictions[1], predictions[2], \n",
    "                 alpha=0.2, label='Intervalo de predicción 95%')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Regresión con Intervalos de Predicción')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# 5. Bondad de Ajuste en Regresión Lineal\n",
    "\n",
    "## 5.1 Coeficiente de Determinación (R²)\n",
    "\n",
    "El R² es una medida fundamental que indica la proporción de la variabilidad en los datos que es explicada por el modelo de regresión.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_r2(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el coeficiente de determinación R².\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Valores observados\n",
    "    y_pred : array-like\n",
    "        Valores predichos\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    float\n",
    "        Valor de R²\n",
    "    dict\n",
    "        Componentes del cálculo (SST, SSR, SSE)\n",
    "    \"\"\"\n",
    "    # Suma total de cuadrados (SST)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    \n",
    "    # Suma de cuadrados de la regresión (SSR)\n",
    "    ss_reg = np.sum((y_pred - np.mean(y))**2)\n",
    "    \n",
    "    # Suma de cuadrados del error (SSE)\n",
    "    ss_res = np.sum((y - y_pred)**2)\n",
    "    \n",
    "    # Cálculo de R²\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    # Alternativamente: r2 = ss_reg / ss_tot\n",
    "    \n",
    "    return r2, {\n",
    "        'SST': ss_tot,\n",
    "        'SSR': ss_reg,\n",
    "        'SSE': ss_res\n",
    "    }\n",
    "\n",
    "def plot_goodness_of_fit(y, y_pred, r2):\n",
    "    \"\"\"\n",
    "    Genera visualizaciones para evaluar la bondad de ajuste.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Valores observados\n",
    "    y_pred : array-like\n",
    "        Valores predichos\n",
    "    r2 : float\n",
    "        Coeficiente de determinación\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gráfico de valores observados vs predichos\n",
    "    ax1.scatter(y, y_pred, alpha=0.5)\n",
    "    ax1.plot([min(y), max(y)], [min(y), max(y)], 'r--', \n",
    "             label='Línea de referencia')\n",
    "    ax1.set_xlabel('Valores Observados')\n",
    "    ax1.set_ylabel('Valores Predichos')\n",
    "    ax1.set_title(f'Observado vs Predicho (R² = {r2:.4f})')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Gráfico Q-Q de residuos\n",
    "    residuals = y - y_pred\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax2)\n",
    "    ax2.set_title('Gráfico Q-Q de Residuos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "r2, components = calculate_r2(y, y_pred)\n",
    "plot_goodness_of_fit(y, y_pred, r2)\n",
    "\n",
    "print(\"\\nComponentes de la variabilidad:\")\n",
    "print(f\"Variabilidad total (SST): {components['SST']:.4f}\")\n",
    "print(f\"Variabilidad explicada (SSR): {components['SSR']:.4f}\")\n",
    "print(f\"Variabilidad residual (SSE): {components['SSE']:.4f}\")\n",
    "print(f\"\\nR² = {r2:.4f}\")\n",
    "```\n",
    "\n",
    "## 5.2 R² Ajustado\n",
    "\n",
    "El R² ajustado penaliza la adición de variables predictoras que no mejoran significativamente el modelo.\n",
    "\n",
    "```python\n",
    "def adjusted_r2(r2, n, p):\n",
    "    \"\"\"\n",
    "    Calcula el R² ajustado.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    r2 : float\n",
    "        R² original\n",
    "    n : int\n",
    "        Número de observaciones\n",
    "    p : int\n",
    "        Número de predictores (sin incluir intercepto)\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    float\n",
    "        R² ajustado\n",
    "    \"\"\"\n",
    "    adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "    return adj_r2\n",
    "\n",
    "# Ejemplo de uso\n",
    "n = len(y)  # número de observaciones\n",
    "p = 1       # número de predictores (sin intercepto)\n",
    "adj_r2 = adjusted_r2(r2, n, p)\n",
    "print(f\"R² ajustado: {adj_r2:.4f}\")\n",
    "```\n",
    "\n",
    "## 5.3 Medidas Adicionales de Ajuste\n",
    "\n",
    "```python\n",
    "def additional_metrics(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula métricas adicionales de bondad de ajuste.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Valores observados\n",
    "    y_pred : array-like\n",
    "        Valores predichos\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict\n",
    "        Diccionario con diferentes métricas\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Error cuadrático medio (MSE)\n",
    "    mse = np.mean(residuals**2)\n",
    "    \n",
    "    # Raíz del error cuadrático medio (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Error absoluto medio (MAE)\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    \n",
    "    # Error porcentual absoluto medio (MAPE)\n",
    "    mape = np.mean(np.abs(residuals / y)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso\n",
    "metrics = additional_metrics(y, y_pred)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "source_map": [
   11,
   15,
   25,
   75,
   93,
   253,
   276,
   451,
   465,
   472,
   480
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}